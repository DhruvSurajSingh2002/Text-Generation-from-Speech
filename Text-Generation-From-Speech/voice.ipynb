{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53cee9a2-6688-4e74-a5ab-d07c13892414",
   "metadata": {},
   "source": [
    "%pip install vosk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6629678-d45b-4dd8-a212-ce58ab4b5807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vosk import Model, KaldiRecognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8529cd3a-0cdb-44c4-9b44-3bdb3c3c8413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:11:12:13:14:15\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /Users/vik/.cache/vosk/vosk-model-en-us-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from /Users/vik/.cache/vosk/vosk-model-en-us-0.22/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:294) Loading words from /Users/vik/.cache/vosk/vosk-model-en-us-0.22/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo /Users/vik/.cache/vosk/vosk-model-en-us-0.22/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:310) Loading subtract G.fst model from /Users/vik/.cache/vosk/vosk-model-en-us-0.22/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:312) Loading CARPA model from /Users/vik/.cache/vosk/vosk-model-en-us-0.22/rescore/G.carpa\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:318) Loading RNNLM model from /Users/vik/.cache/vosk/vosk-model-en-us-0.22/rnnlm/final.raw\n"
     ]
    }
   ],
   "source": [
    "FRAME_RATE = 16000\n",
    "CHANNELS=1\n",
    "\n",
    "model = Model(model_name=\"vosk-model-en-us-0.22\")\n",
    "# For a smaller download size, use model = Model(model_name=\"vosk-model-small-en-us-0.15\")\n",
    "rec = KaldiRecognizer(model, FRAME_RATE)\n",
    "rec.SetWords(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbfa3f6-eb58-4e38-b4ab-1b089eac9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ad229-3be5-456d-b8b7-0a112dfbbd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a2908b5-bff4-4c43-a871-823166fd61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp3 = AudioSegment.from_mp3(\"conversation.mp3\")\n",
    "mp3 = mp3.set_channels(CHANNELS)\n",
    "mp3 = mp3.set_frame_rate(FRAME_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e3e81d0-0178-483e-9186-76916ffc5fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.AcceptWaveform(mp3.raw_data)\n",
    "result = rec.Result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8ed2892-ce39-4105-9f13-ffebc9726218",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[43mresult\u001b[49m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "text = json.loads(result)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abeea5c5-defc-439a-a0f3-5cccb942410f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtext\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4bb907-6793-4e63-8c96-5250832b28ef",
   "metadata": {},
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3105f0-1785-4e45-bb91-187d17713471",
   "metadata": {},
   "source": [
    "%pip install torch -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82058215-6972-4b52-9a7c-0b363565c257",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m cased \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mcheck_output(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython recasepunc/recasepunc.py predict recasepunc/checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m, shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mtext\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "cased = subprocess.check_output('python recasepunc/recasepunc.py predict recasepunc/checkpoint', shell=True, text=True, input=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b5a0db-2eab-48cc-9090-ea3b4618222e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cased' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcased\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cased' is not defined"
     ]
    }
   ],
   "source": [
    "cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd416309-91ac-4acc-9a7a-d3b29d1051a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voice_recognition(filename):\n",
    "    model = Model(model_name=\"vosk-model-en-us-0.22\")\n",
    "    rec = KaldiRecognizer(model, FRAME_RATE)\n",
    "    rec.SetWords(True)\n",
    "    \n",
    "    mp3 = AudioSegment.from_mp3(filename)\n",
    "    mp3 = mp3.set_channels(CHANNELS)\n",
    "    mp3 = mp3.set_frame_rate(FRAME_RATE)\n",
    "    \n",
    "    step = 45000\n",
    "    transcript = \"\"\n",
    "    for i in range(0, len(mp3), step):\n",
    "        print(f\"Progress: {i/len(mp3)}\")\n",
    "        segment = mp3[i:i+step]\n",
    "        rec.AcceptWaveform(segment.raw_data)\n",
    "        result = rec.Result()\n",
    "        text = json.loads(result)[\"text\"]\n",
    "        transcript += text\n",
    "    \n",
    "    cased = subprocess.check_output('python recasepunc/recasepunc.py predict recasepunc/checkpoint', shell=True, text=True, input=transcript)\n",
    "    return cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eec81c65-2708-4b59-96f1-831a02c5e16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:11:12:13:14:15\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /Users/vik/.cache/vosk/vosk-model-en-us-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from /Users/vik/.cache/vosk/vosk-model-en-us-0.22/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:294) Loading words from /Users/vik/.cache/vosk/vosk-model-en-us-0.22/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo /Users/vik/.cache/vosk/vosk-model-en-us-0.22/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:310) Loading subtract G.fst model from /Users/vik/.cache/vosk/vosk-model-en-us-0.22/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:312) Loading CARPA model from /Users/vik/.cache/vosk/vosk-model-en-us-0.22/rescore/G.carpa\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:318) Loading RNNLM model from /Users/vik/.cache/vosk/vosk-model-en-us-0.22/rnnlm/final.raw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.0\n",
      "Progress: 0.02666815218151411\n",
      "Progress: 0.05333630436302822\n",
      "Progress: 0.08000445654454233\n",
      "Progress: 0.10667260872605644\n",
      "Progress: 0.13334076090757055\n",
      "Progress: 0.16000891308908466\n",
      "Progress: 0.18667706527059877\n",
      "Progress: 0.21334521745211288\n",
      "Progress: 0.240013369633627\n",
      "Progress: 0.2666815218151411\n",
      "Progress: 0.29334967399665524\n",
      "Progress: 0.3200178261781693\n",
      "Progress: 0.34668597835968346\n",
      "Progress: 0.37335413054119754\n",
      "Progress: 0.4000222827227117\n",
      "Progress: 0.42669043490422576\n",
      "Progress: 0.4533585870857399\n",
      "Progress: 0.480026739267254\n",
      "Progress: 0.5066948914487681\n",
      "Progress: 0.5333630436302822\n",
      "Progress: 0.5600311958117963\n",
      "Progress: 0.5866993479933105\n",
      "Progress: 0.6133675001748246\n",
      "Progress: 0.6400356523563386\n",
      "Progress: 0.6667038045378528\n",
      "Progress: 0.6933719567193669\n",
      "Progress: 0.720040108900881\n",
      "Progress: 0.7467082610823951\n",
      "Progress: 0.7733764132639093\n",
      "Progress: 0.8000445654454234\n",
      "Progress: 0.8267127176269374\n",
      "Progress: 0.8533808698084515\n",
      "Progress: 0.8800490219899657\n",
      "Progress: 0.9067171741714798\n",
      "Progress: 0.9333853263529939\n",
      "Progress: 0.960053478534508\n",
      "Progress: 0.9867216307160221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: reverting to cpu as cuda is not available\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "transcript = voice_recognition(\"marketplace_full.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "957d3aef-9c37-421f-acfc-21111d5158df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "# For a smaller model, use: summarizer = pipeline(\"summarization\", model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e3b982-da04-4f17-9b49-28bdcf8c2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_tokens = transcript.split(\" \")\n",
    "docs = []\n",
    "for i in range(0, len(split_tokens), 850):\n",
    "    selection = \" \".join(split_tokens[i:(i+850)])\n",
    "    docs.append(selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f75311e-95fb-43eb-a068-9c17e99f1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = summarizer(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d61cc49-72e4-44e6-9fa2-91eb169ba0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\\n\\n\".join([d[\"summary_text\"] for d in summaries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff0d8cd7-000f-419f-98ee-b57b747b01db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msummary\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'summary' is not defined"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    " To tell you basically what this is about is when I was watching Harvey Mackay at one of Harv Eker's things, he said he just finished the Boston marathon and you know, the guy is 76 and I went holy crap, you know, that is amazing. He looked so fit and he is so quick minded and so on I thought, all of a sudden it occurred to me I bet the way you eat, you know, is different. I bet you don't just eat a bunch of garbage and that started this thought. So, the basic three questions will be and I am recording it for you as well if I transcribe these for the book, but then I write about it and what has really been neat about it is that what started out as three same questions to everybody, everybody had kind of a different angle on it and I realized that they were creating the chapters for this book and of course Marci Shimoff read me right [???], I am not doing something where I did all the work and you are just transcribing it, but if you actually write in the book, I will do it. So I made her that promise and it was a hard promise, but it was a good one to make because it made me think more, you know. Got you.\n",
    " So, what I would do is I basically introduce you and then you can add anything that you think is important to that introduction and let me get my history up here because I have you on here. So, how is Robby doing?\n",
    " Good, hangin' in there.\n",
    "Yeah, did you guys have a nice holiday?Well, we actually kind of had a  holiday, her father who is very old got sick and ended up passing away.\n",
    "Oh I am sorry to hear that.But, you know, stuff happens, what are you going to do?\n",
    " So I am going to – is your best website, at the end I am going to ask you, you know, about your website and stuff, is rickfrishman.com the best one to go to or -\n",
    "Yeah probably just for most stuff that is probably the best way to go yeah.\n",
    "You had a really good bio on one of your websites.\n",
    "It is up there, there is one, you know, in most of them. I also have rickfrishmanblog.com, you know.\n",
    "Let me check that out, okay so the -\n",
    "There is a bio on that one, but it is also a bio on just rickfrishman.com.\n",
    "There we go about Rick, yeah.\n",
    "Sure.So you know, one of the things that I will bring up is, you know, you always talk about how you have the biggest Rolodex and I thought that was a really cool angle too because part of success is who you know and you know, I think that is important. I don't know what your angle is going to be on this, but you know, the questions will be do you think that that hypothesis is true that, you know, food affects your ability to succeed on some level and then if you -\n",
    "Food affects your ability to -\n",
    "You know, if it plays into your level of success. In other words, you know, I know there are successful people who eat crappy food, but so far kind of the consensus has been, you know, it has run the gamut of extremes, but so far people seem to say, you know, they can't keep up their energy if you speak a lot. You do a lot of speaking so you know, and you have a hectic schedule, so I imagine that if you are, you know, full of two pizzas, you probably don't have the energy on stage that you normally would.\n",
    "Right, it is true.So, that's kind of the angle, but."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd36710-4a70-4a18-bf2f-a1416425fda1",
   "metadata": {},
   "source": [
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a39ae4-24b9-485f-aa27-8d80945fcd16",
   "metadata": {},
   "source": [
    "!pip install pyaudio\n",
    "!pip install --global-option='build_ext' --global-option='-I/usr/local/include' --global-option='-L/usr/local/lib' pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f56e3a20-03de-4aee-83e1-ac48a4e9c2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LG HDR 4K\n",
      "Cam Link 4K\n",
      "Shure MV7\n",
      "MacBook Pro Microphone\n",
      "MacBook Pro Speakers\n",
      "AirPods Pro\n",
      "AirPods Pro\n",
      "Microsoft Teams Audio\n",
      "ZoomAudioDevice\n"
     ]
    }
   ],
   "source": [
    "p = pyaudio.PyAudio()\n",
    "p.get_device_count()\n",
    "\n",
    "for i in range(p.get_device_count()):\n",
    "    print(p.get_device_info_by_index(i).get('name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17bd52c8-cec8-486b-9877-965c629f3285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "\n",
    "def record_microphone(seconds=10, chunk=1024, audio_format=pyaudio.paInt16):\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=audio_format,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=FRAME_RATE,\n",
    "                    input=True,\n",
    "                    input_device_index=2,\n",
    "                    frames_per_buffer=chunk)\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(FRAME_RATE / chunk * seconds)):\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    sound = AudioSegment(\n",
    "        data=b''.join(frames),\n",
    "        sample_width=p.get_sample_size(audio_format),\n",
    "        frame_rate=FRAME_RATE,\n",
    "        channels=CHANNELS\n",
    "    )\n",
    "    sound.export(\"temp.mp3\", \"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a06fd31e-f969-4e7a-b992-4f18b68581db",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_microphone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ebd4b5-bc8d-48f7-8746-ba81376fb6a1",
   "metadata": {},
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa5976-0982-4a64-b3aa-0c3d16c7e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!brew install portaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c1a352-b0ab-44a7-bbab-9ff0abb03794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37e23e414ae4f34b1795c959d91620c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Record', icon='microphone', style=ButtonStyle(), tooltip='Record')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e527244102644028dbfe577de07c46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "record_button = widgets.Button(\n",
    "    description='Record',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    tooltip='Record',\n",
    "    icon='microphone'\n",
    ")\n",
    "\n",
    "summary = widgets.Output()\n",
    "\n",
    "def start_recording(data):\n",
    "    with summary:\n",
    "        display(\"Starting the recording.\")\n",
    "        record_microphone()\n",
    "        display(\"Finished recording.\")\n",
    "        transcript = voice_recognition(\"temp.mp3\")\n",
    "        display(f\"Transcript: {transcript}\")\n",
    "\n",
    "record_button.on_click(start_recording)\n",
    "\n",
    "display(record_button, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc00ac6-7a29-433d-8464-c83982ebabe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
